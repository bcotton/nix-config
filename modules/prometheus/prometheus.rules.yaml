groups:
# Recording rules for ZFS capacity planning and performance metrics
- name: zfs_recording_rules
  interval: 300s  # 5 minutes
  rules:
    - record: zfs:pool_capacity_percent
      expr: (zfs_pool_allocated_bytes / (zfs_pool_allocated_bytes + zfs_pool_free_bytes)) * 100
    
    - record: zfs:pool_growth_rate_daily
      expr: rate(zfs_pool_allocated_bytes[24h])
    
    - record: zfs:pool_days_until_full
      expr: zfs_pool_free_bytes / (rate(zfs_pool_allocated_bytes[7d]) * 86400)
    
    - record: zfs:dataset_growth_rate_hourly
      expr: rate(zfs_dataset_used_bytes[1h])
    
    - record: zfs:arc_hit_ratio
      expr: zfs_arc_hits / (zfs_arc_hits + zfs_arc_misses)
    
    - record: zfs:arc_memory_utilization
      expr: zfs_arc_size / zfs_arc_c_max

- name: alerts
  rules:
  # Dead Man's Switch - always firing alert to ensure monitoring is working
  - alert: Watchdog
    expr: vector(1)
    labels:
      severity: none
    annotations:
      summary: "An alert that should always be firing to certify that Alertmanager is working properly."
      description: |
        This is an alert meant to ensure that the entire alerting pipeline is functional.
        This alert is always firing, therefore it should always be firing in Alertmanager
        and always fire against a receiver. This integrates with healthchecks.io to monitor
        the monitoring system itself.
  - alert: BorgmaticMissingBackup
    expr: time() - borg_last_backup_timestamp{job="borgmatic"} > 90000
    for: 1m
    labels:
      severity: warning
    annotations:
      summary: "Borg missing backup"
      description: "The instance {{ $labels.instance }} has not created a backup of the repo {{ $labels.repository }} in the last {{ $value }} seconds"
  - alert: HostDown
    expr: up{job="node"} == 0
    for: 15m
    labels:
      severity: critical
    annotations:
      description: '{{ $labels.instance }} is down for more than 15 minutes'
  - alert: HostSystemdServiceCrashed
    expr: (node_systemd_unit_state{state="failed",name!="systemd-networkd-wait-online.service"} == 1) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    labels:
      severity: warning
    annotations:
      description: "Service {{ $labels.name }} crashed"
  - alert: HostRaidDiskFailure
    expr: (node_md_disks{state="failed"} > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    for: 2m
    labels:
      severity: error
    annotations:
      description: "At least one device in RAID array on {{ $labels.instance }} failed. Array {{ $labels.md_device }} needs attention and possibly a disk swap."
  - alert: HostOomKillDetected
    expr: (increase(node_vmstat_oom_kill[1m]) > 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    labels:
      severity: warning
    annotations:
      description: "OOM kill detected"
  # - alert: HostNetworkPacketDropEgress
  #   expr: rate(node_network_transmit_drop_total{device!="flannel.1"}[2m]) > 0
  #   for: 5m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: '{{ $labels.instance }} drops packages for more than 5 minutes'
  # - alert: HostNetworkPacketDropIngress
  #   expr: rate(node_network_receive_drop_total[2m]) > 0.5
  #   for: 5m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: '{{ $labels.instance }} drops packages for more than 5 minutes'
  - alert: HostNetworkBondDegraded
    expr: ((node_bonding_active - node_bonding_slaves) != 0) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "Bond {{ $labels.device }} degraded on {{ $labels.instance }}."
  - alert: HostNetworkInterfaceSaturated
    expr: ((rate(node_network_receive_bytes_total{device!~"^tap.*|^vnet.*|^veth.*|^tun.*"}[1m]) + rate(node_network_transmit_bytes_total{device!~"^tap.*|^vnet.*|^veth.*|^tun.*"}[1m])) / node_network_speed_bytes{device!~"^tap.*|^vnet.*|^veth.*|^tun.*"} > 0.8 < 10000) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    for: 10m
    labels:
      severity: warning
    annotations:
      description: "The network interface {{ $labels.device }} on {{ $labels.instance }} is getting overloaded."
  - alert: HostNetworkReceiveErrors
    expr: (rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} receive errors in the last two minutes."
  - alert: HostNetworkTransmitErrors
    expr: (rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01) * on(instance) group_left (nodename) node_uname_info{nodename=~".+"}
    for: 2m
    labels:
      severity: warning
    annotations:
      description: "Host {{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf \"%.0f\" $value }} transmit errors in the last two minutes."
  - alert: BlackboxProbeFailed
    for: 15m
    expr: probe_success == 0
    labels:
      severity: critical
    annotations:
      description: "Probe failed"

  - alert: DnsResolutionFailed
    expr: probe_success{job="dns_resolution", check="nas-01.lan"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "DNS resolution failed for nas-01.lan"
      description: "nas-01.lan is not resolving to 192.168.5.42 on dns-01. This can cause service connectivity issues."

  - alert: ZfsPoolDegraded
    expr: zfs_pool_health > 0
    for: 2m
    labels:
      severity: critical
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} is degraded. State: {{ $value }}'

  - alert: ZfsPoolHighUsage
    expr: (zfs_pool_allocated_bytes / (zfs_pool_allocated_bytes + zfs_pool_free_bytes)) * 100 > 80
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} is at {{ printf "%.1f" $value }}% capacity'

  - alert: ZfsPoolCriticalUsage
    expr: (zfs_pool_allocated_bytes / (zfs_pool_allocated_bytes + zfs_pool_free_bytes)) * 100 > 90
    for: 10m
    labels:
      severity: critical
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} is critically full at {{ printf "%.1f" $value }}% capacity'

  - alert: ZfsPoolFragmentation
    expr: zfs_pool_fragmentation_ratio > 0.7
    for: 30m
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} has high fragmentation ratio of {{ printf "%.2f" $value }}'

  - alert: ZfsPoolLeaked
    expr: zfs_pool_leaked_bytes > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} has {{ printf "%.0f" $value }} bytes of leaked data'

  # Enhanced ZFS Monitoring - Sanoid/Syncoid Service Monitoring
  - alert: SanoidServiceFailed
    expr: node_systemd_unit_state{name="sanoid.service",state="failed"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Sanoid snapshot service failed on {{ $labels.instance }}'

  - alert: SyncoidReplicationFailed  
    expr: node_systemd_unit_state{name=~"syncoid-.*\\.service",state="failed"} == 1
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Syncoid replication {{ $labels.name }} failed on {{ $labels.instance }}'

  - alert: SanoidServiceNotRunRecently
    expr: (time() - node_systemd_unit_start_time_seconds{name="sanoid.service"}) > 7200
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'Sanoid service has not run in over 2 hours on {{ $labels.instance }}'

  - alert: SyncoidServiceNotRunRecently
    expr: (time() - node_systemd_unit_start_time_seconds{name=~"syncoid-.*\\.service"}) > 86400
    for: 30m
    labels:
      severity: warning
    annotations:
      description: 'Syncoid service {{ $labels.name }} has not run in over 24 hours on {{ $labels.instance }}'

  # ZFS Scrub Monitoring
  - alert: ZfsScrubOverdue
    expr: (time() - zfs_pool_last_scrub_timestamp) > 2592000  # 30 days
    for: 1h
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} on {{ $labels.instance }} has not been scrubbed in over 30 days'

  - alert: ZfsScrubErrors
    expr: zfs_pool_scrub_errors > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'ZFS scrub found {{ $value }} errors in pool {{ $labels.pool }} on {{ $labels.instance }}'

  - alert: ZfsScrubInProgress
    expr: zfs_pool_scrub_state == 1
    for: 24h  # Alert if scrub takes more than 24 hours
    labels:
      severity: warning
    annotations:
      description: 'ZFS scrub has been running for over 24 hours on pool {{ $labels.pool }} on {{ $labels.instance }}'

  # Dataset-Level Monitoring
  # - alert: ZfsDatasetQuotaExceeded
  #   expr: (zfs_dataset_used_bytes / zfs_dataset_quota_bytes) > 0.9
  #   for: 10m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: 'ZFS dataset {{ $labels.dataset }} is at {{ printf "%.1f" $value }}% of quota on {{ $labels.instance }}'

  # - alert: ZfsDatasetRapidGrowth
  #   expr: rate(zfs_dataset_used_bytes[1h]) > 1073741824  # 1GB/hour
  #   for: 30m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: 'ZFS dataset {{ $labels.dataset }} growing rapidly at {{ printf "%.2f" $value }} bytes/hour on {{ $labels.instance }}'

  # - alert: ZfsDatasetHighUsage
  #   expr: (zfs_dataset_used_bytes / zfs_dataset_available_bytes) > 0.85
  #   for: 15m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: 'ZFS dataset {{ $labels.dataset }} is {{ printf "%.1f" $value }}% full on {{ $labels.instance }}'

  # ZFS ARC (Adaptive Replacement Cache) Monitoring
  - alert: ZfsArcLowHitRatio
    expr: (zfs_arc_hits / (zfs_arc_hits + zfs_arc_misses)) < 0.8
    for: 15m
    labels:
      severity: warning
    annotations:
      description: 'ZFS ARC hit ratio is low ({{ printf "%.2f" $value }}) on {{ $labels.instance }}'

  - alert: ZfsArcMemoryPressure
    expr: (zfs_arc_size / zfs_arc_c_max) > 0.95
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'ZFS ARC memory usage is high ({{ printf "%.1f" $value }}%) on {{ $labels.instance }}'

  - alert: ZfsArcMetadataLimit
    expr: (zfs_arc_meta_used / zfs_arc_meta_limit) > 0.9
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'ZFS ARC metadata usage is high ({{ printf "%.1f" $value }}%) on {{ $labels.instance }}'

  # SMART Disk Health Monitoring
  - alert: SmartDeviceFailurePredicted
    expr: smartctl_device_smart_healthy == 0
    for: 0m  # Immediate alert
    labels:
      severity: critical
    annotations:
      description: 'SMART predicts failure for device {{ $labels.device }} on {{ $labels.instance }} - REPLACE IMMEDIATELY'

  - alert: SmartDeviceHighTemperature
    expr: smartctl_device_temperature > 60
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'Device {{ $labels.device }} temperature is high ({{ $value }}°C) on {{ $labels.instance }} - Check cooling'

  - alert: SmartDeviceCriticalTemperature
    expr: smartctl_device_temperature > 70
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Device {{ $labels.device }} temperature is critical ({{ $value }}°C) on {{ $labels.instance }} - IMMEDIATE ACTION REQUIRED'

  # Reallocated Sectors - Progressive Severity (using raw values)
  - alert: SmartDeviceReallocatedSectorsLow
    expr: smartctl_device_attribute{attribute_name="Reallocated_Sector_Ct",attribute_value_type="raw"} > 0 and smartctl_device_attribute{attribute_name="Reallocated_Sector_Ct",attribute_value_type="raw"} <= 5
    for: 5m
    labels:
      severity: warning
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} reallocated sectors on {{ $labels.instance }} - Monitor closely, consider replacement planning'

  - alert: SmartDeviceReallocatedSectorsHigh
    expr: smartctl_device_attribute{attribute_name="Reallocated_Sector_Ct",attribute_value_type="raw"} > 5
    for: 0m
    labels:
      severity: critical
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} reallocated sectors on {{ $labels.instance }} - PLAN IMMEDIATE REPLACEMENT'

  # Pending Sectors - Sectors waiting to be reallocated (URGENT) (using raw values)
  - alert: SmartDevicePendingSectorsLow
    expr: smartctl_device_attribute{attribute_name="Current_Pending_Sector",attribute_value_type="raw"} > 0 and smartctl_device_attribute{attribute_name="Current_Pending_Sector",attribute_value_type="raw"} <= 5
    for: 2m  # Shorter wait time - this is urgent
    labels:
      severity: warning
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} pending sectors on {{ $labels.instance }} - IMMEDIATE ACTION: Run extended SMART test or ZFS scrub to force reallocation'

  - alert: SmartDevicePendingSectorsHigh
    expr: smartctl_device_attribute{attribute_name="Current_Pending_Sector",attribute_value_type="raw"} > 5
    for: 0m  # Immediate alert
    labels:
      severity: critical
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} pending sectors on {{ $labels.instance }} - CRITICAL: Active media failure, plan immediate replacement'

  - alert: SmartDevicePendingSectorsPersistent
    expr: smartctl_device_attribute{attribute_name="Current_Pending_Sector",attribute_value_type="raw"} > 0
    for: 24h  # If pending sectors persist for 24 hours
    labels:
      severity: critical
    annotations:
      description: 'Device {{ $labels.device }} has persistent pending sectors ({{ $value }}) for >24h on {{ $labels.instance }} - Reallocation failing, replace drive immediately'

  # Uncorrectable Sectors - Data loss has occurred (using raw values)
  - alert: SmartDeviceUncorrectableSectors
    expr: smartctl_device_attribute{attribute_name="Offline_Uncorrectable",attribute_value_type="raw"} > 0
    for: 0m
    labels:
      severity: critical
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} uncorrectable sectors on {{ $labels.instance }} - DATA LOSS POSSIBLE, REPLACE IMMEDIATELY'

  # Spin Retry Count - Mechanical drive health (using raw values)
  - alert: SmartDeviceSpinRetryCount
    expr: smartctl_device_attribute{attribute_name="Spin_Retry_Count",attribute_value_type="raw"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      description: 'Device {{ $labels.device }} has {{ $value }} spin retry events on {{ $labels.instance }} - Mechanical issues detected'

  # Raw Read Error Rate - Media degradation (using raw values)
  # - alert: SmartDeviceRawReadErrorRate
  #   expr: smartctl_device_attribute{attribute_name="Raw_Read_Error_Rate",attribute_value_type="raw"} > 1000000
  #   for: 15m
  #   labels:
  #     severity: warning
  #   annotations:
  #     description: 'Device {{ $labels.device }} has high raw read error rate ({{ $value }}) on {{ $labels.instance }} - Media degradation'

  # SSD Specific - Wear Leveling Count (using raw values)
  - alert: SmartSsdWearLevelingLow
    expr: smartctl_device_attribute{attribute_name="Wear_Leveling_Count",attribute_value_type="raw"} < 10 and smartctl_device_attribute{attribute_name="Wear_Leveling_Count",attribute_value_type="raw"} > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'SSD {{ $labels.device }} wear leveling count is low ({{ $value }}) on {{ $labels.instance }} - Plan replacement'

  # SSD Specific - Program/Erase Cycles (using raw values)
  - alert: SmartSsdProgramFailCount
    expr: smartctl_device_attribute{attribute_name="Program_Fail_Cnt_Total",attribute_value_type="raw"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      description: 'SSD {{ $labels.device }} has {{ $value }} program failures on {{ $labels.instance }} - Monitor closely'

  # ZFS Pool I/O Error Monitoring
  - alert: ZfsPoolReadErrors
    expr: rate(zfs_pool_read_errors[5m]) > 0
    for: 2m
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} experiencing read errors on {{ $labels.instance }}'

  - alert: ZfsPoolWriteErrors
    expr: rate(zfs_pool_write_errors[5m]) > 0
    for: 2m
    labels:
      severity: warning
    annotations:
      description: 'ZFS pool {{ $labels.pool }} experiencing write errors on {{ $labels.instance }}'

  - alert: ZfsPoolChecksumErrors
    expr: rate(zfs_pool_checksum_errors[5m]) > 0
    for: 2m
    labels:
      severity: critical
    annotations:
      description: 'ZFS pool {{ $labels.pool }} experiencing checksum errors on {{ $labels.instance }}'

  - alert: BleSensorOffline
    expr: homeassistant_sensor_unit_seconds{domain="sensor",entity="sensor.main_ble_sensor_age",instance="homeassistant:8123"} > 5000
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'BLE sensor {{ $labels.entity }} on {{ $labels.instance }} is not responding.'
  - alert: CondoBleSensorOffline
    expr: homeassistant_sensor_unit_seconds{domain="sensor",entity="sensor.main_ble_sensor_age",instance="condo-ha:8123"} > 5000
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Condo BLE sensor {{ $labels.entity }} on {{ $labels.instance }} is not responding.'
  - alert: Main Temp too Low
    expr: sum(homeassistant_sensor_temperature_celsius{friendly_name="Main Temperature", instance="homeassistant:8123"}) - sum(homeassistant_climate_target_temperature_celsius{friendly_name="Main Thermostat", instance="homeassistant:8123"}) < -1
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Main temp is colder than set point by {{ printf "%.0f" $value }}C. Cycle the hot water.'

  # Nix Cache Proxy Disk Space Monitoring (nas-01 only)
  - alert: NixCacheProxyHighUsage
    expr: (node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} - node_filesystem_avail_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"}) / node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} * 100 > 80
    for: 10m
    labels:
      severity: warning
    annotations:
      description: 'Nix cache proxy on {{ $labels.instance }} is at {{ printf "%.1f" $value }}% capacity'

  - alert: NixCacheProxyCriticalUsage
    expr: (node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} - node_filesystem_avail_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"}) / node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} * 100 > 90
    for: 10m
    labels:
      severity: critical
    annotations:
      description: 'Nix cache proxy on {{ $labels.instance }} is critically full at {{ printf "%.1f" $value }}% capacity'

  - alert: NixCacheProxyFull
    expr: (node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} - node_filesystem_avail_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"}) / node_filesystem_size_bytes{instance="nas-01",mountpoint="/ssdpool/local/nix-cache-proxy"} * 100 > 95
    for: 5m
    labels:
      severity: critical
    annotations:
      description: 'Nix cache proxy on {{ $labels.instance }} is nearly full at {{ printf "%.1f" $value }}% capacity - nginx may fail to cache new packages'

  # Nix Build and Cache Infrastructure Monitoring
  - alert: NixCacheNotAccessible
    expr: nix_cache_info_accessible == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Nix cache is not accessible"
      description: "The Nix binary cache has been unreachable for 5 minutes on {{ $labels.instance }}"

  - alert: NixDistributedBuildsNotWorking
    expr: nix_distributed_build_success == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Nix distributed builds are not working"
      description: "Distributed builds have failed on {{ $labels.instance }}"

  - alert: NixCacheNotHitting
    expr: nix_local_cache_hit == 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Nix cache is not serving hits"
      description: "The Nix cache is not serving successful cache hits on {{ $labels.instance }}"

  - alert: NixUpstreamCacheProxyDown
    expr: nix_upstream_cache_accessible == 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Nix upstream cache proxy is not accessible"
      description: "The nginx proxy to cache.nixos.org is not accessible on {{ $labels.instance }}"

  - alert: NixBuildCacheCheckStale
    expr: (time() - nix_build_cache_check_timestamp) > 1800
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Nix build cache check is stale"
      description: "The Nix build and cache validation has not run in over 30 minutes on {{ $labels.instance }}"

  # Garage S3 Object Storage Monitoring
  - alert: GarageDown
    expr: up{job="garage"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Garage S3 service is down"
      description: "Garage admin API on {{ $labels.instance }} has been unreachable for 5 minutes"

  - alert: GarageClusterUnhealthy
    expr: cluster_healthy == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Garage cluster is unhealthy"
      description: "Garage cluster on {{ $labels.instance }} reports unhealthy status"

  - alert: GarageClusterUnavailable
    expr: cluster_available == 0
    for: 2m
    labels:
      severity: critical
    annotations:
      summary: "Garage cluster is unavailable"
      description: "Garage cluster on {{ $labels.instance }} is not available for read/write operations"

  - alert: GarageDiskSpaceLow
    expr: (garage_local_disk_avail{volume="data"} / garage_local_disk_total{volume="data"}) < 0.2
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Garage data disk space low"
      description: "Garage data volume on {{ $labels.instance }} has less than 20% disk space remaining"

  - alert: GarageDiskSpaceCritical
    expr: (garage_local_disk_avail{volume="data"} / garage_local_disk_total{volume="data"}) < 0.1
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Garage data disk space critical"
      description: "Garage data volume on {{ $labels.instance }} has less than 10% disk space remaining"

  - alert: GarageMetadataDiskSpaceLow
    expr: (garage_local_disk_avail{volume="metadata"} / garage_local_disk_total{volume="metadata"}) < 0.2
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Garage metadata disk space low"
      description: "Garage metadata volume on {{ $labels.instance }} has less than 20% disk space remaining"

  - alert: GarageHighS3ErrorRate
    expr: rate(api_s3_error_counter[5m]) > 1
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Garage S3 high error rate"
      description: "Garage on {{ $labels.instance }} is returning S3 errors at {{ printf \"%.1f\" $value }}/sec"

  - alert: GarageBlockResyncErrors
    expr: block_resync_errored_blocks > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Garage has block resync errors"
      description: "Garage on {{ $labels.instance }} has {{ $value }} errored blocks in resync"

  - alert: GarageBlockResyncQueueHigh
    expr: block_resync_queue_length > 1000
    for: 30m
    labels:
      severity: warning
    annotations:
      summary: "Garage block resync queue is large"
      description: "Garage on {{ $labels.instance }} has {{ $value }} blocks in the resync queue"

  # Grafana Mimir Monitoring
  - alert: MimirDown
    expr: up{job="mimir"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Mimir is down"
      description: "Mimir on {{ $labels.instance }} has been unreachable for 5 minutes"

  - alert: MimirIngestionFailures
    expr: rate(cortex_ingester_ingested_samples_failures_total{job="mimir"}[5m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Mimir sample ingestion failures"
      description: "Mimir on {{ $labels.instance }} is failing to ingest samples at {{ printf \"%.1f\" $value }}/sec"

  - alert: MimirCompactionFailed
    expr: increase(cortex_compactor_runs_failed_total{job="mimir"}[1h]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Mimir compaction failures"
      description: "Mimir compactor on {{ $labels.instance }} has had failed runs in the last hour"

  - alert: MimirHighMemoryUsage
    expr: process_resident_memory_bytes{job="mimir"} / 1073741824 > 4
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Mimir high memory usage"
      description: "Mimir on {{ $labels.instance }} is using {{ printf \"%.1f\" $value }}GB of memory"

  - alert: MimirRequestErrors
    expr: rate(cortex_request_duration_seconds_count{job="mimir",status_code=~"5.."}[5m]) > 0.5
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Mimir returning server errors"
      description: "Mimir on {{ $labels.instance }} is returning 5xx errors at {{ printf \"%.1f\" $value }}/sec for {{ $labels.route }}"

  - alert: MimirRemoteWriteFailing
    expr: rate(prometheus_remote_storage_failed_samples_total{remote_name="mimir"}[5m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus remote write to Mimir is failing"
      description: "Prometheus is failing to send samples to Mimir at {{ printf \"%.1f\" $value }}/sec"

  - alert: MimirRemoteWriteQueueFull
    expr: prometheus_remote_storage_shards_max{remote_name="mimir"} - prometheus_remote_storage_shards{remote_name="mimir"} <= 0
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Prometheus remote write to Mimir queue is at max shards"
      description: "All remote write shards to Mimir are in use - samples may be dropped"

  - alert: MimirBlockLoadFailures
    expr: increase(cortex_bucket_store_block_load_failures_total{job="mimir"}[1h]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Mimir S3 block load failures"
      description: "Mimir on {{ $labels.instance }} is failing to load blocks from S3 storage"

  # Grafana Loki Monitoring
  - alert: LokiDown
    expr: up{job="loki"} == 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Loki is down"
      description: "Loki on {{ $labels.instance }} has been unreachable for 5 minutes"

  - alert: LokiIngestionFailures
    expr: rate(loki_ingester_chunks_flushed_total{job="loki",reason="error"}[5m]) > 0
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Loki chunk flush failures"
      description: "Loki on {{ $labels.instance }} is failing to flush chunks at {{ printf \"%.1f\" $value }}/sec"

  - alert: LokiRequestErrors
    expr: rate(loki_request_duration_seconds_count{job="loki",status_code=~"5.."}[5m]) > 0.5
    for: 10m
    labels:
      severity: warning
    annotations:
      summary: "Loki returning server errors"
      description: "Loki on {{ $labels.instance }} is returning 5xx errors at {{ printf \"%.1f\" $value }}/sec for {{ $labels.route }}"

  - alert: LokiHighMemoryUsage
    expr: process_resident_memory_bytes{job="loki"} / 1073741824 > 4
    for: 15m
    labels:
      severity: warning
    annotations:
      summary: "Loki high memory usage"
      description: "Loki on {{ $labels.instance }} is using {{ printf \"%.1f\" $value }}GB of memory"

  - alert: LokiCompactorFailed
    expr: increase(loki_compactor_runs_failed_total{job="loki"}[1h]) > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Loki compaction failures"
      description: "Loki compactor on {{ $labels.instance }} has had failed runs in the last hour"

  - alert: LokiIngesterWALCorruption
    expr: increase(loki_ingester_wal_corruptions_total{job="loki"}[1h]) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Loki WAL corruption detected"
      description: "Loki ingester on {{ $labels.instance }} has detected WAL corruption"

  - alert: LokiPanics
    expr: increase(loki_panic_total{job="loki"}[1h]) > 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Loki panics detected"
      description: "Loki on {{ $labels.instance }} has experienced {{ $value }} panics in the last hour"

  - alert: LokiRingUnhealthy
    expr: loki_ring_members{job="loki",state="Unhealthy"} > 0
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "Loki ring has unhealthy members"
      description: "Loki ring on {{ $labels.instance }} has {{ $value }} unhealthy members"